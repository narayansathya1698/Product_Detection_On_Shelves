{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Product_Detection_Position_Recognition.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMiiKLpOu82gV7t7ybd516d",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/narayansathya1698/Product_Detection_On_Shelves/blob/master/Product_Detection_Position_Recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MyEErk_GTFij",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tensorflow==1.7.0\n",
        "!pip install keras==2.1.6"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9VmjD6jITLnD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://github.com/gulvarol/grocerydataset/releases/download/1.0/GroceryDataset_part1.tar.gz\n",
        "!wget https://github.com/gulvarol/grocerydataset/releases/download/1.0/GroceryDataset_part2.tar.gz\n",
        "!tar -xvzf GroceryDataset_part1.tar.gz\n",
        "!tar -xvzf GroceryDataset_part2.tar.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MqIhlmMeVRLB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir classifier\n",
        "!wget http://paleobots.com/classifier/environment.tar.gz\n",
        "!tar --strip-components 1 -xzf environment.tar.gz\n",
        "!rm environment.tar.gz\n",
        "!apt-get install -y axel imagemagick > /dev/null\n",
        "!echo Environment is instantiated"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVJmEJ-nYPZd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd \"/content/data/images/ShelfImages\" \n",
        "!mogrify -rotate 180 C1_P03_N1_S2_2.JPG\n",
        "!mogrify -rotate 180 C1_P03_N1_S3_1.JPG\n",
        "!mogrify -rotate 180 C1_P03_N1_S3_2.JPG\n",
        "!mogrify -rotate 180 C1_P03_N1_S4_1.JPG\n",
        "!mogrify -rotate 180 C1_P03_N1_S4_2.JPG\n",
        "!mogrify -rotate 180 C1_P03_N2_S3_1.JPG\n",
        "!mogrify -rotate 180 C1_P03_N2_S3_2.JPG\n",
        "!mogrify -rotate 180 C1_P03_N3_S2_1.JPG\n",
        "!mogrify -rotate 180 C1_P12_N1_S2_1.JPG\n",
        "!mogrify -rotate 180 C1_P12_N1_S3_1.JPG\n",
        "!mogrify -rotate 180 C1_P12_N1_S4_1.JPG\n",
        "!mogrify -rotate 180 C1_P12_N1_S5_1.JPG\n",
        "!mogrify -rotate 180 C1_P12_N2_S2_1.JPG\n",
        "!mogrify -rotate 180 C1_P12_N2_S3_1.JPG\n",
        "!mogrify -rotate 180 C1_P12_N2_S4_1.JPG\n",
        "!mogrify -rotate 180 C1_P12_N2_S5_1.JPG\n",
        "!mogrify -rotate 180 C1_P12_N3_S2_1.JPG\n",
        "!mogrify -rotate 180 C1_P12_N3_S3_1.JPG\n",
        "!mogrify -rotate 180 C1_P12_N3_S4_1.JPG\n",
        "!mogrify -rotate 180 C1_P12_N4_S2_1.JPG\n",
        "!mogrify -rotate 180 C1_P12_N4_S3_1.JPG\n",
        "!mogrify -rotate -90 C3_P07_N1_S6_1.JPG"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dl-KS7qicu0d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get install -y -qq protobuf-compiler python-pil python-lxml"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4gn-_ElGe3mv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone --quiet https://github.com/tensorflow/models.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4L3rw3Le8wO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.chdir('models/research')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ep-0lxqZfARP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!protoc object_detection/protos/*.proto --python_out=."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLe3sxZ1fD1y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.append('/content/models/research/slim')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUnwSGTBfMFb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6R8W2W5fTbf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_path = '/content/data/'\n",
        "# we'll use data from two folders\n",
        "shelf_images = '/content/data/images/ShelfImages/'\n",
        "product_images = '/content/data/images/ProductImagesFromShelves/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AG3D2HvMfa52",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# let's get all shelves photo data from ShelfImages\n",
        "jpg_files = [f for f in os.listdir(f'{shelf_images}') if f.endswith('JPG')]\n",
        "photos_df = pd.DataFrame([[f, f[:6], f[7:14]] for f in jpg_files], \n",
        "                         columns=['file', 'shelf_id', 'planogram_id'])\n",
        "photos_df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-E9i1w2fdxR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#let's get products on shelves photo from ProductImagesFromShelves\n",
        "products_df = pd.DataFrame(\n",
        "    [[f[:18], f[:6], f[7:14], i, *map(int, f[19:-4].split('_'))] \n",
        "     for i in range(11) \n",
        "     for f in os.listdir(f'{product_images}{i}') if f.endswith('png')],\n",
        "    columns=['file', 'shelf_id', 'planogram_id', \n",
        "             'category', 'xmin', 'ymin', 'w', 'h'])\n",
        "# convert from width height to xmax, ymax\n",
        "products_df['xmax'] = products_df['xmin'] + products_df['w']\n",
        "products_df['ymax'] = products_df['ymin'] + products_df['h']\n",
        "products_df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Amkczou1fwVt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get distinct shelves\n",
        "shelves = list(set(photos_df['shelf_id'].values))\n",
        "# use train_test_split from sklearn\n",
        "shelves_train, shelves_validation, _, _ = train_test_split(\n",
        "    shelves, shelves, test_size=0.3, random_state=6)\n",
        "# mark all records in data frames with is_train flag\n",
        "def is_train(shelf_id): return shelf_id in shelves_train\n",
        "photos_df['is_train'] = photos_df.shelf_id.apply(is_train)\n",
        "products_df['is_train'] = products_df.shelf_id.apply(is_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RNbwpOHuf5eR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = products_df[products_df.category != 0].\\\n",
        "         groupby(['category', 'is_train'])['category'].\\\n",
        "         count().unstack('is_train').fillna(0)\n",
        "df.plot(kind='barh', stacked=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3sjxIqNf89P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save to pkl\n",
        "photos_df.to_pickle(f'{data_path}photos.pkl')\n",
        "products_df.to_pickle(f'{data_path}products.pkl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1EuhjnmmgCNe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# function to display shelf photo with rectangled products\n",
        "def draw_shelf_photo(file):\n",
        "    file_products_df = products_df[products_df.file == file]\n",
        "    coordinates = file_products_df[['xmin', 'ymin', 'xmax', 'ymax']].values\n",
        "    im = cv2.imread(f'{shelf_images}{file}')\n",
        "    im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)    \n",
        "    for xmin, ymin, xmax, ymax in coordinates:\n",
        "        cv2.rectangle(im, (xmin, ymin), (xmax, ymax), (0, 255, 0), 5)\n",
        "    plt.imshow(im)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67Q_LQz0gFMQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# draw one photo to check our data\n",
        "fig = plt.gcf()\n",
        "fig.set_size_inches(18.5, 10.5)\n",
        "draw_shelf_photo('C3_P07_N1_S6_1.JPG')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iv1WE1W4gKqf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from IPython.display import Image\n",
        "Image('docs/images/brands.png', width=300)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0VDOCxELmQ-M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import itertools\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, Input\n",
        "from keras.layers import Conv2D, MaxPooling2D, AveragePooling2D\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.regularizers import l2\n",
        "from keras.models import Model\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "from keras import backend as K\n",
        "\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0VvlmCUGmUrc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_path = '/content/data/'\n",
        "shelf_images = '/content/data/images/ShelfImages/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCXgx_cHma7e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "photos_df = pd.read_pickle(f'{data_path}photos.pkl')\n",
        "products_df =  pd.read_pickle(f'{data_path}products.pkl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6ZWycwqmeaA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_classes = 10\n",
        "SHAPE_WIDTH = 80\n",
        "SHAPE_HEIGHT = 120"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aAb3Ile4mhCv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# resize pack to fixed size SHAPE_WIDTH x SHAPE_HEIGHT\n",
        "def resize_pack(pack):\n",
        "    fx_ratio = SHAPE_WIDTH / pack.shape[1]\n",
        "    fy_ratio = SHAPE_HEIGHT / pack.shape[0]    \n",
        "    pack = cv2.resize(pack, (0, 0), fx=fx_ratio, fy=fy_ratio)\n",
        "    return pack[0:SHAPE_HEIGHT, 0:SHAPE_WIDTH]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EmJEtnCjmkKR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x, y, f = [], [], []\n",
        "for file, is_train in photos_df[['file', 'is_train']].values:\n",
        "    photo_rects = products_df[products_df.file == file]\n",
        "    rects_data = photo_rects[['category', 'xmin', 'ymin', 'xmax', 'ymax']]\n",
        "    im = cv2.imread(f'{shelf_images}{file}')\n",
        "    im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n",
        "    for category, xmin, ymin, xmax, ymax in rects_data.values:\n",
        "        if category == 0:\n",
        "            continue\n",
        "        pack = resize_pack(np.array(im[ymin:ymax, xmin:xmax]))\n",
        "        x.append(pack)\n",
        "        f.append(is_train)\n",
        "        y.append(category - 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOhIHFxVmm1c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.imshow(x[60])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXpVvl9cmxgT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# let's split the data to train/validation sets based on our is_train flag\n",
        "x = np.array(x)\n",
        "y = np.array(y)\n",
        "f = np.array(f)\n",
        "x_train, x_validation, y_train, y_validation = x[f], x[~f], y[f], y[~f]\n",
        "# save validation images\n",
        "x_validation_images = x_validation"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOYh51Yqm1Cd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# convert y_train and y_validation to one-hot arrays\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_validation = keras.utils.to_categorical(y_validation, num_classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eEdJwE8fm4sS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# normalize x_train, x_validation\n",
        "x_train = x_train.astype('float32')\n",
        "x_validation = x_validation.astype('float32')\n",
        "x_train /= 255\n",
        "x_validation /= 255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jD5S_FDhm7S7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# let's see what do we have\n",
        "print('x_train shape:', x_train.shape)\n",
        "print('y_train shape:', y_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_validation.shape[0], 'validation samples')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yszi9Pv1nD0D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# let's build our ResNet CNN. We don't do any significant changes to keras example\n",
        "def lr_schedule(epoch):\n",
        "    lr = 1e-3\n",
        "    if epoch > 5:\n",
        "        lr *= 1e-1\n",
        "    print('Learning rate: ', lr)\n",
        "    return lr\n",
        "\n",
        "\n",
        "def resnet_layer(inputs,\n",
        "                 num_filters=16,\n",
        "                 kernel_size=3,\n",
        "                 strides=1,\n",
        "                 activation='relu',\n",
        "                 batch_normalization=True,\n",
        "                 conv_first=True):\n",
        "    conv = Conv2D(num_filters,\n",
        "                  kernel_size=kernel_size,\n",
        "                  strides=strides,\n",
        "                  padding='same',\n",
        "                  kernel_initializer='he_normal',\n",
        "                  kernel_regularizer=l2(1e-4))\n",
        "\n",
        "    x = inputs\n",
        "    if conv_first:\n",
        "        x = conv(x)\n",
        "        if batch_normalization:\n",
        "            x = BatchNormalization()(x)\n",
        "        if activation is not None:\n",
        "            x = Activation(activation)(x)\n",
        "    else:\n",
        "        if batch_normalization:\n",
        "            x = BatchNormalization()(x)\n",
        "        if activation is not None:\n",
        "            x = Activation(activation)(x)\n",
        "        x = conv(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "def resnet_v1(input_shape, depth, num_classes=10):\n",
        "    if (depth - 2) % 6 != 0:\n",
        "        raise ValueError('depth should be 6n+2 (eg 20, 32, 44 in [a])')\n",
        "    # Start model definition.\n",
        "    num_filters = 16\n",
        "    num_res_blocks = int((depth - 2) / 6)\n",
        "\n",
        "    inputs = Input(shape=x_train.shape[1:])\n",
        "    x = resnet_layer(inputs=inputs)\n",
        "    # Instantiate the stack of residual units\n",
        "    for stack in range(3):\n",
        "        for res_block in range(num_res_blocks):\n",
        "            strides = 1\n",
        "            if stack > 0 and res_block == 0:  # first layer but not first stack\n",
        "                strides = 2  # downsample\n",
        "            y = resnet_layer(inputs=x,\n",
        "                             num_filters=num_filters,\n",
        "                             strides=strides)\n",
        "            y = resnet_layer(inputs=y,\n",
        "                             num_filters=num_filters,\n",
        "                             activation=None)\n",
        "            if stack > 0 and res_block == 0:  # first layer but not first stack\n",
        "                # linear projection residual shortcut connection to match\n",
        "                # changed dims\n",
        "                x = resnet_layer(inputs=x,\n",
        "                                 num_filters=num_filters,\n",
        "                                 kernel_size=1,\n",
        "                                 strides=strides,\n",
        "                                 activation=None,\n",
        "                                 batch_normalization=False)\n",
        "            x = keras.layers.add([x, y])\n",
        "            x = Activation('relu')(x)\n",
        "        num_filters *= 2\n",
        "\n",
        "    # Add classifier on top.\n",
        "    # v1 does not use BN after last shortcut connection-ReLU\n",
        "    x = AveragePooling2D(pool_size=8)(x)\n",
        "    y = Flatten()(x)\n",
        "    outputs = Dense(num_classes,\n",
        "                    activation='softmax',\n",
        "                    kernel_initializer='he_normal')(y)\n",
        "\n",
        "    # Instantiate model.\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "n = 3\n",
        "version = 1\n",
        "if version == 1:\n",
        "    depth = n * 6 + 2\n",
        "elif version == 2:\n",
        "    depth = n * 9 + 2\n",
        "model_type = 'ResNet%dv%d' % (depth, version)\n",
        "\n",
        "model = resnet_v1(input_shape=x_train.shape[1:], depth=depth, num_classes=num_classes)\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Adam(lr=lr_schedule(0)), metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4idbIRgnnXlN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tz7oAeYopXXe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This will do preprocessing and realtime data augmentation:\n",
        "datagen = ImageDataGenerator(\n",
        "    featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "    samplewise_center=False,  # set each sample mean to 0\n",
        "    featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "    samplewise_std_normalization=False,  # divide each input by its std\n",
        "    zca_whitening=False,  # apply ZCA whitening\n",
        "    rotation_range=5,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "    width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
        "    height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
        "    horizontal_flip=False,  # randomly flip images\n",
        "    vertical_flip=False)  # randomly flip images\n",
        "datagen.fit(x_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iWBrNC9KpcTk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 50\n",
        "epochs = 15\n",
        "model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
        "                    validation_data=(x_validation, y_validation),\n",
        "                    epochs=epochs, verbose=1, workers=4,steps_per_epoch=40, \n",
        "                    callbacks=[LearningRateScheduler(lr_schedule)])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZEd_j-jpgKX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# let's estimate our result\n",
        "scores = model.evaluate(x_validation, y_validation, verbose=1)\n",
        "print('Test loss:', scores[0])\n",
        "print('Test accuracy:', scores[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTw3eebOTMSH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_confusion_matrix(cm, classes, normalize=False, \n",
        "                          title='Confusion matrix', cmap=plt.cm.Blues):\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8R0InqBmTycC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# let's draw confusion matrix to check classes recognition performance\n",
        "y_validation_cls = np.argmax(y_validation, axis=1)\n",
        "y_validation_predict = model.predict(x_validation)\n",
        "y_validation_predict_cls = np.argmax(y_validation_predict, axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QzVNnHI2T2Ps",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig = plt.gcf()\n",
        "fig.set_size_inches(10, 10)\n",
        "cnf_matrix = confusion_matrix(y_validation_cls, y_validation_predict_cls)\n",
        "plot_confusion_matrix(cnf_matrix, [f'C{i+1}' for i in range(num_classes)], \n",
        "                      title='Confusion matrix', normalize=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8OQVbE5UUmi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "power = np.array([y_validation_predict[i][y_validation_predict_cls[i]] \n",
        "                  for i in range(len(y_validation_predict_cls))])\n",
        "\n",
        "\n",
        "margin = 5\n",
        "width = num_classes * SHAPE_WIDTH + (num_classes - 1) * margin\n",
        "height = num_classes * SHAPE_HEIGHT + (num_classes - 1) * margin\n",
        "confusion_image = np.zeros((height, width, 3), dtype='i')\n",
        "for i in range(num_classes):\n",
        "    for j in range(num_classes):\n",
        "        flags = [(y_validation_cls == i) & (y_validation_predict_cls == j)]\n",
        "        if not np.any(flags):\n",
        "            continue\n",
        "        max_cell_power = np.max(power[flags])\n",
        "        index = np.argmax(flags & (power == max_cell_power))\n",
        "        ymin, xmin = (SHAPE_HEIGHT+margin) * i, (SHAPE_WIDTH+margin) * j\n",
        "        ymax, xmax = ymin + SHAPE_HEIGHT, xmin + SHAPE_WIDTH\n",
        "        confusion_image[ymin:ymax, xmin:xmax, :] = x_validation_images[index]\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOLMGj0iUZS9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig = plt.gcf()\n",
        "fig.set_size_inches(20, 20)\n",
        "plt.imshow(confusion_image)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHQ5SzVBUcoY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def deprocess_image(x):\n",
        "    x -= x.mean()\n",
        "    x /= (x.std() + 1e-5)\n",
        "    x *= 0.1\n",
        "    x += 0.5\n",
        "    x = np.clip(x, 0, 1)\n",
        "    x *= 255\n",
        "    x = np.clip(x, 0, 255).astype('uint8')\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7w_7Qk5UlFd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n = 4\n",
        "\n",
        "margin = 5\n",
        "width = n * SHAPE_WIDTH + (n - 1) * margin\n",
        "height = n * SHAPE_HEIGHT + (n - 1) * margin\n",
        "stitched_filters = np.zeros((height, width, 3), dtype='i')\n",
        "\n",
        "\n",
        "input_img = model.input\n",
        "# get the symbolic outputs of each \"key\" layer (we gave them unique names).\n",
        "layer_dict = dict([(layer.name, layer) for layer in model.layers])\n",
        "layer_name = 'conv2d_5'\n",
        "for i in range(n):\n",
        "    for j in range(n):\n",
        "        filter_index = i * n + j\n",
        "        layer_output = layer_dict[layer_name].output\n",
        "        loss = K.mean(layer_output[:, :, :, filter_index])\n",
        "        grads = K.gradients(loss, input_img)[0]\n",
        "        grads /= (K.sqrt(K.mean(K.square(grads))) + 1e-5)\n",
        "        iterate = K.function([input_img], [loss, grads])\n",
        "        input_img_data = np.random.random((1, SHAPE_HEIGHT, SHAPE_WIDTH, 3))\n",
        "        input_img_data = input_img_data * 20 + 128.\n",
        "        step = 1.\n",
        "        for k in range(20):\n",
        "            loss_value, grads_value = iterate([input_img_data])\n",
        "            input_img_data += grads_value * step\n",
        "        img = deprocess_image(input_img_data[0])\n",
        "        ymin, xmin = (SHAPE_HEIGHT+margin) * i, (SHAPE_WIDTH+margin) * j\n",
        "        ymax, xmax = ymin + SHAPE_HEIGHT, xmin + SHAPE_WIDTH\n",
        "        stitched_filters[ymin:ymax, xmin:xmax, :] = img\n",
        "\n",
        "fig = plt.gcf()\n",
        "fig.set_size_inches(15, 15)\n",
        "plt.imshow(stitched_filters)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9RXJXNeaUowp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import io\n",
        "import tensorflow as tf\n",
        "\n",
        "from PIL import Image\n",
        "from object_detection.utils import dataset_util\n",
        "from collections import namedtuple, OrderedDict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGPTgFnpV68e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# current images path \n",
        "img_path = '/content/data/images/ShelfImages/'\n",
        "# cropped parts destination\n",
        "cropped_path = '/content/data/images/detector/'\n",
        "# Step 1 results path\n",
        "data_path = '/content/data/'\n",
        "# output destination\n",
        "detector_data_path = '/content/pack_detector/data/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GhVlM9_8XQfq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# read rects and photos dataframes\n",
        "photos = pd.read_pickle(f'{data_path}photos.pkl')\n",
        "products = pd.read_pickle(f'{data_path}products.pkl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v5awqg63XVPd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "N_CROP_TRIALS = 6\n",
        "CROP_SIZE = 1000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3aDHO16oXYmm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# returns random value in [s, f]\n",
        "def rand_between(s, f):\n",
        "    if s == f:\n",
        "        return s\n",
        "    return np.random.randint(s, f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ttjPqANHXbCe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_products, eval_products = [], []\n",
        "for img_file, is_train in photos[['file', 'is_train']].values:\n",
        "    img = cv2.imread(f'{img_path}{img_file}')\n",
        "    img_h, img_w, img_c = img.shape\n",
        "    for n in range(N_CROP_TRIALS):\n",
        "        # randomly crop square\n",
        "        c_size = rand_between(300, max(img_h, img_w))\n",
        "        x0 = rand_between(0, max(0, img_w - c_size))\n",
        "        y0 = rand_between(0, max(0, img_h - c_size))\n",
        "        x1 = min(img_w, x0 + c_size)\n",
        "        y1 = min(img_h, y0 + c_size)\n",
        "        # products totally inside crop rectangle\n",
        "        crop_products = products[(products.file == img_file) & \n",
        "                                 (products.xmin > x0) & (products.xmax < x1) &\n",
        "                                 (products.ymin > y0) & (products.ymax < y1)]\n",
        "        # no products inside crop rectangle? cropping trial failed...\n",
        "        if len(crop_products) == 0:\n",
        "            continue\n",
        "        # name the crop\n",
        "        crop_img_file = f'{img_file[:-4]}{x0}_{y0}_{x1}_{y1}.JPG'\n",
        "        # crop and reshape to CROP_SIZExCROP_SIZE or smaller \n",
        "        # keeping aspect ratio\n",
        "        crop = img[y0:y1, x0:x1]\n",
        "        h, w, c = crop.shape\n",
        "        ratio = min(CROP_SIZE/h, CROP_SIZE/w)\n",
        "        crop = cv2.resize(crop, (0,0), fx=ratio, fy=ratio)\n",
        "        crop = crop[0:CROP_SIZE, 0:CROP_SIZE]\n",
        "        h, w, c = crop.shape\n",
        "        # add crop inner products to train_products or eval_products list\n",
        "        for xmin, ymin, xmax, ymax in \\\n",
        "                crop_products[['xmin', 'ymin', 'xmax', 'ymax']].values:\n",
        "            xmin -= x0\n",
        "            xmax -= x0\n",
        "            ymin -= y0\n",
        "            ymax -= y0\n",
        "\n",
        "            xmin, xmax, ymin, ymax = [int(np.round(e * ratio)) \n",
        "                                      for e in [xmin, xmax, ymin, ymax]]\n",
        "            product = {'filename': crop_img_file, 'class':'pack', \n",
        "                       'width':w, 'height':h,\n",
        "                       'xmin':xmin, 'ymin':ymin, 'xmax':xmax, 'ymax':ymax}\n",
        "            if is_train:\n",
        "                train_products.append(product)\n",
        "            else:\n",
        "                eval_products.append(product)\n",
        "        # save crop top eval or train folder\n",
        "        subpath = ['eval/', 'train/'][is_train]\n",
        "        cv2.imwrite(f'{cropped_path}{subpath}{crop_img_file}', crop)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUhFZrYdXfGE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df = pd.DataFrame(train_products).set_index('filename')\n",
        "eval_df = pd.DataFrame(eval_products).set_index('filename')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDJ2IPigYFSP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# at this point we have two folders with images and\n",
        "# two dataframes with information about packs\n",
        "train_df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dtjHF-sYMAi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def class_text_to_int(row_label):\n",
        "    if row_label == 'pack':\n",
        "        return 1\n",
        "    else:\n",
        "        None\n",
        "\n",
        "\n",
        "def split(df, group):\n",
        "    data = namedtuple('data', ['filename', 'object'])\n",
        "    gb = df.groupby(group)\n",
        "    return [data(filename, gb.get_group(x)) \n",
        "            for filename, x in zip(gb.groups.keys(), gb.groups)]\n",
        "\n",
        "\n",
        "def create_tf_example(group, path):\n",
        "    with tf.gfile.GFile(os.path.join(path, '{}'.format(group.filename)), 'rb') as fid:\n",
        "        encoded_jpg = fid.read()\n",
        "    encoded_jpg_io = io.BytesIO(encoded_jpg)\n",
        "    image = Image.open(encoded_jpg_io)\n",
        "    width, height = image.size\n",
        "\n",
        "    filename = group.filename.encode('utf8')\n",
        "    image_format = b'jpg'\n",
        "    xmins = []\n",
        "    xmaxs = []\n",
        "    ymins = []\n",
        "    ymaxs = []\n",
        "    classes_text = []\n",
        "    classes = []\n",
        "\n",
        "    for index, row in group.object.iterrows():\n",
        "        xmins.append(row['xmin'] / width)\n",
        "        xmaxs.append(row['xmax'] / width)\n",
        "        ymins.append(row['ymin'] / height)\n",
        "        ymaxs.append(row['ymax'] / height)\n",
        "        classes_text.append(row['class'].encode('utf8'))\n",
        "        classes.append(class_text_to_int(row['class']))\n",
        "\n",
        "    tf_example = tf.train.Example(features=tf.train.Features(feature={\n",
        "        'image/height': dataset_util.int64_feature(height),\n",
        "        'image/width': dataset_util.int64_feature(width),\n",
        "        'image/filename': dataset_util.bytes_feature(filename),\n",
        "        'image/source_id': dataset_util.bytes_feature(filename),\n",
        "        'image/encoded': dataset_util.bytes_feature(encoded_jpg),\n",
        "        'image/format': dataset_util.bytes_feature(image_format),\n",
        "        'image/object/bbox/xmin': dataset_util.float_list_feature(xmins),\n",
        "        'image/object/bbox/xmax': dataset_util.float_list_feature(xmaxs),\n",
        "        'image/object/bbox/ymin': dataset_util.float_list_feature(ymins),\n",
        "        'image/object/bbox/ymax': dataset_util.float_list_feature(ymaxs),\n",
        "        'image/object/class/text': dataset_util.bytes_list_feature(classes_text),\n",
        "        'image/object/class/label': dataset_util.int64_list_feature(classes),\n",
        "    }))\n",
        "    return tf_example"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmOHFE4KYYb3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert_to_tf_records(images_path, examples, dst_file):\n",
        "    writer = tf.python_io.TFRecordWriter(dst_file)\n",
        "    grouped = split(examples, 'filename')\n",
        "    for group in grouped:\n",
        "        tf_example = create_tf_example(group, images_path)\n",
        "        writer.write(tf_example.SerializeToString())\n",
        "    writer.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kb6vq8VlYbL5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "convert_to_tf_records(f'{cropped_path}train/', train_df, f'/content/pack_detector/data/train.record')\n",
        "convert_to_tf_records(f'{cropped_path}eval/', eval_df, f'/content/pack_detector/data/eval.record')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnnXuOdNYdju",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import cv2\n",
        "import pandas as pd\n",
        "\n",
        "from collections import defaultdict\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# This is needed since the notebook is stored in the object_detection folder.\n",
        "from object_detection.utils import ops as utils_ops\n",
        "from object_detection.utils import label_map_util\n",
        "from object_detection.utils import visualization_utils as vis_util\n",
        "\n",
        "if tf.__version__ < '1.4.0':\n",
        "    raise ImportError('Please upgrade your tensorflow installation to v1.4.* or later!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bqoaq4kBZawU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This is needed to display the images.\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tu0v9O-vZdbW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# paths to main folders: with frozen graph, with classes labels, \n",
        "# with all shelves images and with data\n",
        "PATH_TO_MODEL = '/content/pack_detector_fg/frozen_inference_graph.pb'\n",
        "PATH_TO_LABELS = '/content/pack_detector/data/pack.pbtxt'\n",
        "PATH_TO_IMAGES = '/content/data/images/ShelfImages/'\n",
        "PATH_TO_DATA = '/content/data/'\n",
        "NUM_CLASSES = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKqE9WZIZm-u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load photos dataframe to get all evaluation images names\n",
        "photos = pd.read_pickle(f'{PATH_TO_DATA}photos.pkl')\n",
        "photos = photos[~photos.is_train]\n",
        "photos.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgM5lvVJZpIU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load frozen graph\n",
        "detection_graph = tf.Graph()\n",
        "with detection_graph.as_default():\n",
        "    od_graph_def = tf.GraphDef()\n",
        "    with tf.gfile.GFile(PATH_TO_MODEL, 'rb') as fid:\n",
        "        serialized_graph = fid.read()\n",
        "        od_graph_def.ParseFromString(serialized_graph)\n",
        "        tf.import_graph_def(od_graph_def, name='')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5MasNULgZr24",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# it is useful to be able to run inference not only on the whole image,\n",
        "# but also on its parts\n",
        "# cutoff - minimum detection scrore needed to take box\n",
        "def run_inference_for_image_part(image_tensor, sess, tensor_dict, \n",
        "                                 image, cutoff, ax0, ay0, ax1, ay1):\n",
        "    boxes = []\n",
        "    im = image[ay0:ay1, ax0:ax1]\n",
        "    h, w, c = im.shape\n",
        "    output_dict = run_inference_for_single_image(im, image_tensor, sess, tensor_dict)\n",
        "    for i in range(100):\n",
        "        if output_dict['detection_scores'][i] < cutoff:\n",
        "            break\n",
        "        y0, x0, y1, x1, score = *output_dict['detection_boxes'][i], \\\n",
        "                                output_dict['detection_scores'][i]\n",
        "        x0, y0, x1, y1, score = int(x0*w), int(y0*h), \\\n",
        "                                int(x1*w), int(y1*h), \\\n",
        "                                int(score * 100)\n",
        "        boxes.append((x0+ax0, y0+ay0, x1+ax0, y1+ay0, score))\n",
        "    return boxes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZzvXya9hn9v-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# additional helper function to work not with coordinates but with percents\n",
        "def run_inference_for_image_part_pcnt(image_tensor, sess, tensor_dict, \n",
        "                                 image, cutoff, p_ax0, p_ay0, p_ax1, p_ay1):\n",
        "    h, w, c = image.shape\n",
        "    max_x, max_y = w-1, h-1\n",
        "    return run_inference_for_image_part(\n",
        "                                image_tensor, sess, tensor_dict, \n",
        "                                image, cutoff, \n",
        "                                int(p_ax0*max_x), int(p_ay0*max_y), \n",
        "                                int(p_ax1*max_x), int(p_ay1*max_y))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nj6TeHBen-tm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# function to display image with bounding boxes\n",
        "def display_image_with_boxes(image, boxes, p_x0=0, p_y0=0, p_x1=1, p_y1=1):\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    for x0, y0, x1, y1, score in boxes:\n",
        "        image = cv2.rectangle(image, (x0, y0), (x1, y1), (0,255,0), 5)\n",
        "    if p_x0 != 0 or p_y0 !=0 or p_x1 != 1 or p_y1 != 1:\n",
        "        h, w, c = image.shape\n",
        "        max_x, max_y = w-1, h-1\n",
        "        image = cv2.rectangle(image, \n",
        "                              (int(p_x0*max_x), int(p_y0*max_y)), \n",
        "                              (int(p_x1*max_x), int(p_y1*max_y)), (0,0,255), 5)\n",
        "    plt.figure(figsize=(14, 14))\n",
        "    plt.imshow(image)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ez-QS5krn-W2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# initializations function\n",
        "def initialize_graph():\n",
        "    ops = tf.get_default_graph().get_operations()\n",
        "    all_tensor_names = {output.name\n",
        "                        for op in ops\n",
        "                        for output in op.outputs}\n",
        "    tensor_dict = {}\n",
        "    for key in ['num_detections', 'detection_boxes',\n",
        "                'detection_scores', 'detection_classes',\n",
        "                'detection_masks']:\n",
        "        tensor_name = key + ':0'\n",
        "        if tensor_name in all_tensor_names:\n",
        "            tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(tensor_name)\n",
        "    image_tensor = tf.get_default_graph().get_tensor_by_name('image_tensor:0')\n",
        "    return image_tensor, tensor_dict\n",
        "\n",
        "# starting function for inference\n",
        "def do_inference_and_display(file, cutoff, p_x0=0, p_y0=0, p_x1=1, p_y1=1):\n",
        "    with detection_graph.as_default():\n",
        "        with tf.Session() as sess:\n",
        "            image_tensor, tensor_dict = initialize_graph()\n",
        "            image = cv2.imread(f'{PATH_TO_IMAGES}{file}')\n",
        "            h, w, c = image.shape\n",
        "            boxes = run_inference_for_image_part_pcnt(\n",
        "                image_tensor, sess, tensor_dict, image, cutoff, p_x0, p_y0, p_x1, p_y1)\n",
        "            display_image_with_boxes(image, boxes, p_x0, p_y0, p_x1, p_y1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3DVTY-ip1_f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# to save time let's start with really hard image\n",
        "do_inference_and_display('C1_P10_N1_S4_1.JPG', 0.5)\n",
        "# it works not bad, but not with 100% quality"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-kS8hlUp1Uz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# main function for sliding window inference\n",
        "def do_sliding_window_inference(file, cutoff):\n",
        "    with detection_graph.as_default():\n",
        "        with tf.Session() as sess:\n",
        "            image_tensor, tensor_dict = initialize_graph()\n",
        "            image = cv2.imread(f'{PATH_TO_IMAGES}{file}')\n",
        "            h, w, c = image.shape\n",
        "            boxes = run_inference_for_image_part_pcnt(\n",
        "                image_tensor, sess, tensor_dict, image, cutoff, 0, 0, 1, 1)\n",
        "            a = np.array(boxes)\n",
        "            mean_dx = int(np.mean(a[:,2]-a[:,0]))\n",
        "            mean_dy = int(np.mean(a[:,3]-a[:,1]))\n",
        "            step_x, step_y = mean_dx, mean_dy\n",
        "            window_size = 2*mean_dy\n",
        "            boxes = []\n",
        "            y0 = 0\n",
        "            while y0 < h-1:\n",
        "                x0 = 0\n",
        "                while x0 < w-1:\n",
        "                    x1, y1 = x0 + window_size, y0 + window_size\n",
        "                    boxes += run_inference_for_image_part(\n",
        "                        image_tensor, sess, tensor_dict, image, cutoff, \n",
        "                        x0, y0, x1, y1)\n",
        "                    x0 += step_y\n",
        "                y0 += step_x\n",
        "            display_image_with_boxes(image, boxes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nkDZFu8vqkE7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "do_sliding_window_inference('C1_P10_N1_S4_1.JPG', 0.9)\n",
        "# it captures all packs, but we also have a lot of the same pack detections\n",
        "# we need non-maximum suppression"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_XvpfSPBqjxt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# function for non-maximum suppression\n",
        "def non_max_suppression(boxes, overlapThresh):\n",
        "    if len(boxes) == 0:\n",
        "        return np.array([]).astype(\"int\")\n",
        "\n",
        "    if boxes.dtype.kind == \"i\":\n",
        "        boxes = boxes.astype(\"float\")\n",
        " \n",
        "    pick = []\n",
        "\n",
        "    x1 = boxes[:,0]\n",
        "    y1 = boxes[:,1]\n",
        "    x2 = boxes[:,2]\n",
        "    y2 = boxes[:,3]\n",
        "    sc = boxes[:,4]\n",
        " \n",
        "    area = (x2 - x1 + 1) * (y2 - y1 + 1)\n",
        "    idxs = np.argsort(sc)\n",
        " \n",
        "    while len(idxs) > 0:\n",
        "        last = len(idxs) - 1\n",
        "        i = idxs[last]\n",
        "        pick.append(i)\n",
        " \n",
        "        xx1 = np.maximum(x1[i], x1[idxs[:last]])\n",
        "        yy1 = np.maximum(y1[i], y1[idxs[:last]])\n",
        "        xx2 = np.minimum(x2[i], x2[idxs[:last]])\n",
        "        yy2 = np.minimum(y2[i], y2[idxs[:last]])\n",
        " \n",
        "        w = np.maximum(0, xx2 - xx1 + 1)\n",
        "        h = np.maximum(0, yy2 - yy1 + 1)\n",
        "\n",
        "        #todo fix overlap-contains...\n",
        "        overlap = (w * h) / area[idxs[:last]]\n",
        "         \n",
        "        idxs = np.delete(idxs, np.concatenate(([last],\n",
        "            np.where(overlap > overlapThresh)[0])))\n",
        "    \n",
        "    return boxes[pick].astype(\"int\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwFPZu3dqoHV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# main function to \n",
        "def do_sliding_window_inference_with_nm_suppression(file, cutoff):\n",
        "    with detection_graph.as_default():\n",
        "        with tf.Session() as sess:\n",
        "            image_tensor, tensor_dict = initialize_graph()\n",
        "            image = cv2.imread(f'{PATH_TO_IMAGES}{file}')\n",
        "            h, w, c = image.shape\n",
        "            boxes = run_inference_for_image_part_pcnt(\n",
        "                image_tensor, sess, tensor_dict, image, cutoff, 0, 0, 1, 1)\n",
        "            a = np.array(boxes)\n",
        "            mean_dx = int(np.mean(a[:,2]-a[:,0]))\n",
        "            mean_dy = int(np.mean(a[:,3]-a[:,1]))\n",
        "            step_x, step_y = mean_dx, mean_dy\n",
        "            window_size = 2*mean_dy\n",
        "            boxes = []\n",
        "            y0 = 0\n",
        "            while y0 < h-1:\n",
        "                x0 = 0\n",
        "                while x0 < w-1:\n",
        "                    x1, y1 = x0 + window_size, y0 + window_size\n",
        "                    boxes += run_inference_for_image_part(\n",
        "                        image_tensor, sess, tensor_dict, image, cutoff, \n",
        "                        x0, y0, x1, y1)\n",
        "                    x0 += step_y\n",
        "                y0 += step_x\n",
        "            boxes = non_max_suppression(np.array(boxes), 0.5)\n",
        "            display_image_with_boxes(image, boxes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uGOx1V1Dqn6q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "do_sliding_window_inference_with_nm_suppression('C1_P10_N1_S4_1.JPG', 0.9)\n",
        "# it gives perfect result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-ZdGb-6qywj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# some more examples\n",
        "do_sliding_window_inference_with_nm_suppression('C2_P03_N4_S2_1.JPG', 0.9)\n",
        "do_sliding_window_inference_with_nm_suppression('C3_P02_N1_S2_2.JPG', 0.5) # note 0.5!!!\n",
        "do_sliding_window_inference_with_nm_suppression('C3_P02_N3_S2_2.JPG', 0.9)\n",
        "do_sliding_window_inference_with_nm_suppression('C2_P01_N1_S5_1.JPG', 0.9)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6oSZJUPqykk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U81u4vInqyXV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}